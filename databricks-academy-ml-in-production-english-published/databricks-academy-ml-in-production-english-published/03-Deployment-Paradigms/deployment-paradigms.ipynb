{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947b6a8a-bbac-49b9-9050-65abf61d4f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839457b4-d5fa-434e-a5ec-5b13b75989b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "#\n",
    "import mlflow\n",
    "import logging\n",
    "#\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from databricks import feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a63500b-3774-4d72-b6ed-5e1eb0c04f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"mlflow\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71859fc4-f74a-409f-b27a-006182d1bef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diamonds_df = sns.load_dataset('diamonds').drop(columns=['cut', 'clarity', 'color'], axis=1)\n",
    "diamonds_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb8d204-aa41-4564-a802-fe559108d50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "drop duplicates and split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3d12b0-a145-4025-a699-9cee13e0191e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diamonds_sdf = spark.createDataFrame(diamonds_df).dropDuplicates()\n",
    "#\n",
    "# Spark Dataframes\n",
    "test_sdf = diamonds_sdf.orderBy(rand()).limit(int(33*diamonds_sdf.count()/100))\n",
    "train_sdf = diamonds_sdf.subtract(test_sdf)\n",
    "#\n",
    "# Pandas Dataframes\n",
    "test_df = test_sdf.toPandas()\n",
    "train_df = train_sdf.toPandas()\n",
    "#\n",
    "print(f\"Number of rows test set: {test_sdf.count()}\")\n",
    "print(f\"Number of rows train set: {train_sdf.count()}\")\n",
    "print(f\"Sum of count rows of train and test set: {train_sdf.count() + test_sdf.count()}\")\n",
    "print(f\"Total number of rows of initial dataframe: {diamonds_sdf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3444e85c-8c51-4fcb-9fbe-48502f0acd7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Train using scikit learn library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ff3dec-0074-41d0-9f76-98913be885d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features and target dataframes\n",
    "X = train_df.drop('price', axis=1)\n",
    "y = train_df['price']\n",
    "#\n",
    "# train model (is automatically logged to mlflow)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "rf.fit(X, y)\n",
    "#\n",
    "# get latest run_id programmaticaly\n",
    "latest_run_id = mlflow.search_runs().sort_values(by=\"end_time\", ascending=False).head(1)['run_id'][0]\n",
    "#\n",
    "# uri to latest run (by default, artifact_path is 'model')\n",
    "uri_scikit_learn = f\"runs:/{latest_run_id}/model\"\n",
    "#\n",
    "# register latest logged model\n",
    "mlflow.register_model(uri_scikit_learn, name=\"scikit-learn_model\")\n",
    "#\n",
    "# load latest registered model\n",
    "scikit_learn_model = mlflow.pyfunc.load_model(uri_scikit_learn)\n",
    "#\n",
    "# prediction of test set using loaded model\n",
    "pd.DataFrame(scikit_learn_model.predict(test_df.drop('price', axis=1)), columns=['predictions']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4fb810f-ec22-4845-8228-49a1f3352056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Train using MLLIb library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8266c3-4039-4653-81aa-bda3f6752b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set vector assembler parameters\n",
    "assembler_inputs = [c for c in train_sdf.columns if c not in ['price']]\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "#\n",
    "# instantiate model\n",
    "mllib_rfr = LinearRegression(featuresCol=\"features\", labelCol='price')\n",
    "#\n",
    "# define pipeline stages\n",
    "stages = [vec_assembler, mllib_rfr]\n",
    "#\n",
    "# set pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "#\n",
    "# fit pipeline to train set\n",
    "model_mllib = pipeline.fit(train_sdf)\n",
    "#\n",
    "# get latest run_id programmaticaly\n",
    "latest_run_id = mlflow.search_runs().sort_values(by=\"end_time\", ascending=False).head(1)['run_id'][0]\n",
    "#\n",
    "# uri to latest run (by default, artifact_path is 'model')\n",
    "uri_mllib = f\"runs:/{latest_run_id}/model\"\n",
    "#\n",
    "# register latest logged model\n",
    "mlflow.register_model(uri_mllib, name=\"mllib_model\")\n",
    "#\n",
    "# load latest registered model\n",
    "mllib_model = mlflow.pyfunc.load_model(uri_mllib)\n",
    "#\n",
    "# Here predictions can be done using same input as for model trained using scikit learn library\n",
    "pd.DataFrame(mllib_model.predict(test_df.drop('price', axis=1)), columns=['predictions']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde85b14-22cb-4dc2-9729-14e3a93f3271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load model into a spark udf\n",
    "predict_scikit_learn = mlflow.pyfunc.spark_udf(spark, uri_scikit_learn)\n",
    "#\n",
    "# make predictions on the spark test dataframe\n",
    "display(test_sdf.withColumn(\"prediction\", predict_scikit_learn(*[c for c in test_sdf.columns if c not in ['price']])).select(\"price\", \"prediction\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65b0f94-32c0-4ef0-93ce-99a820ca8e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Z-ordering  \n",
    "\n",
    "Z-Ordering: colocates related information in the same set of files\n",
    "\n",
    "Z-Ordering is a form of multi-dimensional clustering that colocates related information in the same set of files. It reduces the amount of data that needs to be read. See more here.\n",
    "\n",
    "Here after is an example of use of Z-ordering.\n",
    "\n",
    "Let's first write a dataframe as a Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40571cd4-3424-4070-9703-96249dd3c33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(train_sdf.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"overwriteSchema\", \"true\")\n",
    "          .saveAsTable(\"train_set_diamonds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07252ff-ad29-4623-9bce-54ca06e24ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"describe table extended train_set_diamonds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b58699-c7bc-4727-aeb1-45f907627d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"describe table extended train_set_diamonds\").filter(\"col_name in ('Location')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77b84be8-a06f-4bab-ba05-7bc07522fc7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Z-Ordering re-organizes the data to cluster on\n",
    "certain columns. For queries frequently filtered by a\n",
    "particular column (e.g., customer_id or date),\n",
    "Z-Ordering reduces I/O by enabling efficient data\n",
    "skipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffcdf958-c579-44e4-ae12-5c8d514ba088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_partitioned_path = \"dbfs:/user/hive/warehouse/train_set_diamonds\"\n",
    "#\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_partitioned_path}` ZORDER BY (carat)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a910339-c38c-4d19-99d9-d46883c4c2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partitioning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962c7000-1938-44ad-810e-731db4ca97d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.createDataFrame(sns.load_dataset('diamonds')).write\n",
    "                                                    .format(\"delta\")\n",
    "                                                    .mode(\"overwrite\")\n",
    "                                                    .option(\"overwriteSchema\", \"true\")\n",
    "                                                    .saveAsTable(\"diamonds_df_not_partitioned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5613ca40-42c8-4d6a-8561-afe00010c7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"describe table extended diamonds_df_not_partitioned\").filter(\"col_name in ('Location')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d12791f-6264-4297-a0ae-afdd4892d348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(\"dbfs:/user/hive/warehouse/diamonds_df_not_partitioned\"):\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b82a21-501e-4d56-88d3-d8dd15e57530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"diamonds_df_not_partitioned\").groupBy(\"cut\").count().orderBy(desc('count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7795dc-29e9-456c-bfbb-7061827f354d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"diamonds_df_not_partitioned\")\n",
    "      .write.partitionBy(\"cut\")\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .saveAsTable(\"diamonds_df_partitioned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798aeecb-b5b5-4d31-8c70-e24a1ff7c415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(\"dbfs:/user/hive/warehouse/diamonds_df_partitioned\"):\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fccc50b0-b02e-42e9-8bc1-99634496b30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Score batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4101207-2c77-4725-85db-e4ee8bc97181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1e6773-26be-4f11-8c3a-3f33a02f552a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_diamonds = sns.load_dataset('diamonds').reset_index()\n",
    "#\n",
    "diamonds_full = spark.createDataFrame(pd_diamonds).withColumnRenamed('x', 'x_r')\n",
    "#\n",
    "display(diamonds_full.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a42a6f5-cd38-4265-bdb8-9862e8191359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "y_test  = diamonds_full.select(\"price\", \"index\").orderBy(rand()).limit(int(33*diamonds_full.count()/100))\n",
    "y_train = diamonds_full.select(\"price\", \"index\").subtract(y_test)\n",
    "#\n",
    "display(y_test.limit(5))\n",
    "display(y_train.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e586fa29-6bf0-42df-b5e1-924b2726ee7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "new_diamond = (diamonds_full.limit(1).withColumn('index',   lit(88887777).cast('long'))\n",
    "                                     .withColumn('carat',   lit(2).cast('double'))\n",
    "                                     .withColumn('cut',     lit('Good').cast('string'))\n",
    "                                     .withColumn('color',   lit('E').cast('string'))\n",
    "                                     .withColumn('clarity', lit('VS1').cast('string'))\n",
    "                                     .withColumn('depth',   lit(40).cast('double'))\n",
    "                                     .withColumn('table',   lit(64).cast('double'))\n",
    "                                     .withColumn('x_r',     lit(4.14).cast('double'))\n",
    "                                     .withColumn('y',       lit(3.5).cast('double'))\n",
    "                                     .withColumn('z',       lit(2.1).cast('double')))\n",
    "#\n",
    "new_diamond_with_price = spark.createDataFrame(pd.DataFrame({'index': [88887777], 'price': [4500]}))\n",
    "#\n",
    "new_diamond_without_price = spark.createDataFrame(pd.DataFrame({'index': [88887777]}))\n",
    "#\n",
    "diamond_unknown = spark.createDataFrame(pd.DataFrame({'index': [98989898]}))\n",
    "#\n",
    "display(new_diamond)\n",
    "display(new_diamond_with_price)\n",
    "display(new_diamond_without_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce1a799f-742d-458e-9026-006543408b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We verify that score_batch predicts either with/without the price of the new data, the only requirement is the primary key - in this particular case, column index - of the new diamond data in the Feature Store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73476501-bef2-4886-a307-a4e616778467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And verify that if a primary key is not found in the Feature Store, it results in an error:"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "deployment-paradigms",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
