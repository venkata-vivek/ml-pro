{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fab4bb-231e-48cf-8ed4-fc15a1b22845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h4 style=\"font-variant-caps: small-caps;font-size:35pt;\">Databricks-ML-professional-S01c-Advanced-Experiment-Tracking</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8710b4bb-7fe6-484e-86a9-ffd51a7d4d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>\n",
    "<br/>\n",
    "<p>This Notebook adds information related to the following requirements:</p><br/>\n",
    "<b>Advanced Experiment Tracking:</b>\n",
    "<ul>\n",
    "<li>Perform MLflow experiment tracking workflows using model signatures and input examples</li>\n",
    "<li>Identify the requirements for tracking nested runs</li>\n",
    "<li>Describe the process of enabling autologging, including with the use of Hyperopt</li>\n",
    "<li>Log and view artifacts like SHAP plots, custom visualizations, feature data, images, and metadata</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<p><b>Download this notebook at format ipynb <a href=\"Databricks-ML-professional-S01c-Advanced-Experiment-Tracking.ipynb\">here</a>.</b></p>\n",
    "<br/>\n",
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f6d0da-1d81-4fa0-9770-a9e4d6863534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">1. Import libraries</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2d2e59-7426-4d5f-8d97-3dcff6e5151d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "#\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, FMRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "import mlflow\n",
    "#\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "#\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c05c1b-015d-405a-b6be-f8484a985d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"mlflow\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa08db2c-a856-4c86-81fe-9a8b7322cd6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">2. Load dataset, convert to Spark DataFrame</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b64ff08-1603-4d0c-bc4e-19c0094c3b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tips_df = sns.load_dataset(\"tips\")\n",
    "#\n",
    "tips_sdf = spark.createDataFrame(tips_df)\n",
    "#\n",
    "display(tips_sdf.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b595b34-0633-4f66-9ca0-6067f4cc0716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">3. Prepare data</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "453316e6-0dc3-41b0-9730-27c39ed9bdf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Some transformations are done to prepare dataset to be used to train a ML model.</p>\n",
    "<table border style='border-collapse: collapse;'>\n",
    "<tr style=\"background-color:#EDEDED\">\n",
    "    <th>column name</th>\n",
    "    <th>comment</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>tip</code></td>\n",
    "    <td><b style='color:orangered'>target</b> to predict. Contains numeric</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>total_bill</code></td>\n",
    "    <td>numeric column to keep as is</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>sex</code></td>\n",
    "    <td>Contains <code>Female</code> and <code>Male</code> converted to <code>0</code> and <code>1</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>smoker</code></td>\n",
    "    <td>Contains <code>yes</code> and <code>no</code> converted to <code>0</code> and <code>1</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>time</code></td>\n",
    "    <td>Contains <code>Dinner</code> and <code>Lunch</code> converted to <code>0</code> and <code>1</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>day</code></td>\n",
    "    <td>categorical column to <b>One Hot Encode</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><code>size</code></td>\n",
    "    <td>categorical column to <b>One Hot Encode</b></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c6fbbf-0a08-4fee-8ad7-abdf5a0f9ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tips_sdf = tips_sdf.selectExpr(\"total_bill\",\n",
    "                               \"tip\",\n",
    "                               \"case when sex = 'Female' then 1 else 0 end as sex\",\n",
    "                               \"case when smoker = 'yes' then 1 else 0 end as smoker\",\n",
    "                               \"case when time = 'Dinner' then 1 else 0 end as time\",\n",
    "                               \"day\",\n",
    "                               \"size\")\n",
    "#\n",
    "train_df, test_df = tips_sdf.randomSplit([.8, .2], seed=42)\n",
    "#\n",
    "ohe_cols = [\"size\", \"day\"]\n",
    "num_cols = [\"total_bill\", \"sex\", \"smoker\", \"time\"]\n",
    "target_col = \"tip\"\n",
    "#\n",
    "string_indexer = StringIndexer(inputCols=ohe_cols, outputCols=[c+\"_index\" for c in ohe_cols], handleInvalid=\"skip\")\n",
    "#\n",
    "ohe = OneHotEncoder()\n",
    "ohe.setInputCols([c+\"_index\" for c in ohe_cols])\n",
    "ohe.setOutputCols([c+\"_ohe\" for c in ohe_cols])\n",
    "#\n",
    "assembler_inputs = [c+\"_ohe\" for c in ohe_cols] + num_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "910af898-da90-4e26-a856-cdb4b902e101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">4. Evaluator and model</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06212c8c-e7bf-45e7-827f-fd3fcad64486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "glr =       GeneralizedLinearRegression(featuresCol=\"features\", labelCol=target_col, maxIter=10)\n",
    "lrm =       LinearRegression(featuresCol=\"features\", labelCol=target_col)\n",
    "fmr =       FMRegressor(featuresCol=\"features\", labelCol=target_col, stepSize=0.001)\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1fc5a1-c77d-45e4-88b2-d2861900b3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"signatureandinputexample\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">5. Perform MLflow experiment tracking workflows using model signatures and input examples</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af34c76-f8fd-40a6-a62a-7dd1a94e88de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "signature = mlflow.models.infer_signature(train_df, train_df[[\"tip\"]]);\n",
    "print(signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40df8662-ab45-49a0-8069-8943078270eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_example = train_df.toPandas().head()\n",
    "input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a1d573-d054-48bb-864a-fb9eab2efaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"linear-regression\"\n",
    "#\n",
    "with mlflow.start_run(run_name=\"Tip-run\") as run:\n",
    "    #\n",
    "    # define pipeline stages according to model\n",
    "    stages = [string_indexer, ohe, vec_assembler, lrm]\n",
    "    #\n",
    "    # set pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    #\n",
    "    # fit pipeline to train set\n",
    "    model = pipeline.fit(train_df)\n",
    "    #\n",
    "    # manually log model to mlflow\n",
    "    mlflow.spark.log_model(model, model_name, signature=signature, input_example=input_example)\n",
    "    #\n",
    "    # manually log parameter to mlflow\n",
    "    mlflow.log_param(\"maxIter\", 11)\n",
    "    #\n",
    "    # predict test set\n",
    "    pred_df = model.transform(test_df)\n",
    "    #\n",
    "    # evaluate prediction\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    #\n",
    "    # manually log metric to mlflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c1ad7c-c381-4758-bb59-5114ba6f0ba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"nestedrun\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">6. Identify the requirements for tracking nested runs</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89edcb39-0b90-44ca-b6fd-5af69c3115a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>It is possible to log to mlflow using nested runs:</p>\n",
    "<a id=\"nestedrun\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5839d28-4117-400d-9a8c-d7fa5fbd0665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"tips_evaluation\") as run_parent:\n",
    "    #\n",
    "    # loop on the three regression models\n",
    "    for regression_model in [glr, lrm, fmr]:\n",
    "        #\n",
    "        # get model name\n",
    "        model_name = regression_model.__str__().split(\"_\")[0]\n",
    "        #\n",
    "        # Nest mlflow logging\n",
    "        with mlflow.start_run(run_name=model_name, nested=True) as run:\n",
    "            #\n",
    "            # define pipeline stages according to model\n",
    "            stages = [string_indexer, ohe, vec_assembler, regression_model]\n",
    "            #\n",
    "            # set pipeline\n",
    "            pipeline = Pipeline(stages=stages)\n",
    "            #\n",
    "            # fit pipeline to train set\n",
    "            model = pipeline.fit(train_df)\n",
    "            #\n",
    "            # log model to mlflow\n",
    "            mlflow.spark.log_model(model, model_name, signature=signature, input_example=input_example)\n",
    "            #\n",
    "            # predict test set\n",
    "            pred_df = model.transform(test_df)\n",
    "            #\n",
    "            # evaluate prediction\n",
    "            rmse = evaluator.evaluate(pred_df)\n",
    "            #\n",
    "            # log evaluation to mlflow\n",
    "            mlflow.log_metric(\"rmse\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04a8cf6-a501-4e11-a7af-66b9b9bd6744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://i.ibb.co/TrSPRZP/mlflow6.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29d945c5-a93c-4f84-a01b-341d71e9f980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"autologhyperopt\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">7. Describe the process of enabling autologging, including with the use of Hyperopt</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ea640ea-2ab2-46f6-b53f-a440ef888681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h5>Here we enable mlflow logging with <code>autolog()</code> and train a simple model. This will automatically log everything possible for each library used.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b75ef74-0a1a-4740-8dc1-567388562b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c28cef57-eb3d-40d6-91f0-24b5ea00505f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Now let's fit and evaluate a model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9b4b6e-18be-4c01-ac94-ddfb7263b97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fit pipeline to train set\n",
    "model_lrm_autolog = pipeline.fit(train_df)\n",
    "#\n",
    "# predict test set\n",
    "pred_df = model_lrm_autolog.transform(test_df)\n",
    "#\n",
    "# evaluate\n",
    "evaluator.evaluate(pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431483d6-48ce-4392-87c2-95dabcfd87c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>After that, in MLflow UI, we can see the many parameters that have been logged.</p>\n",
    "<p>Alternatively, we can get and see the logged parameters for latest run programmaticaly:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb984d77-72af-4a8b-8ae7-14188488963a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_mlflow = (spark.createDataFrame(mlflow.search_runs())\n",
    "                      .drop(*['tags.mlflow.source.name',\n",
    "                              'tags.mlflow.databricks.notebookPath',\n",
    "                              'tags.mlflow.user',\n",
    "                              'tags.mlflow.databricks.workspaceURL',\n",
    "                              'tags.mlflow.databricks.cluster.info']))\n",
    "display(output_mlflow.orderBy(desc(\"end_time\")).limit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54efcfb9-459f-4682-af3a-71d7b9f188bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h5>Let's here use <b>HyperOpt</b> for hyperparameter tuning and use <code>autolog()</code> to log everything.</h5>\n",
    "<p><b>HyperOpt</b>:</p>\n",
    "<ul>\n",
    "<li>Let's first define a function to train each model and log information as nested run of hyperopt analysis in mlflow</li>\n",
    "<li>After that, definition of the objective function to minimize</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720a634b-bc15-41e4-ad84-93ff25a01bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(maxIter, regParam, elasticNetParam, labelCol):\n",
    "    \"\"\"\n",
    "    This train() function:\n",
    "     - takes hyperparameters as inputs (for tuning later)\n",
    "     - returns the rmse score on the test dataset\n",
    "    \"\"\"\n",
    "    # Use MLflow to track training.\n",
    "    # Specify \"nested=True\" since this single model will be logged as a child run of Hyperopt's run.\n",
    "    with mlflow.start_run(nested=True):\n",
    "        #\n",
    "        model_hyperopt = LinearRegression(maxIter=maxIter,\n",
    "                                          regParam=regParam,\n",
    "                                          elasticNetParam=elasticNetParam,\n",
    "                                          labelCol=target_col)\n",
    "        #\n",
    "        evaluator_hyperopt = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "        #\n",
    "        stages = [string_indexer, ohe, vec_assembler, model_hyperopt]\n",
    "        #\n",
    "        # set pipeline\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        #\n",
    "        # fit pipeline to train set\n",
    "        model_rfr_hyperopt = pipeline.fit(train_df)\n",
    "        #\n",
    "        # predict test set\n",
    "        pred_df = model_rfr_hyperopt.transform(test_df)\n",
    "        #\n",
    "        # evaluate\n",
    "        rmse = evaluator_hyperopt.evaluate(pred_df)\n",
    "        #\n",
    "        # log rmse for each child run\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "    #\n",
    "    return model_rfr_hyperopt, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6604cb-3318-4341-93ba-465ea966a13c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\" This function is the function to minimize by hyperopt \"\"\"\n",
    "    #\n",
    "    model, rmse = train_model(maxIter=params[\"maxIter\"],\n",
    "                              regParam=params[\"regParam\"],\n",
    "                              elasticNetParam=params[\"elasticNetParam\"],\n",
    "                              labelCol=target_col)\n",
    "    #\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116f2066-932c-4268-98e0-2fc9ff77e6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's define the hyperparameter search spaces:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edfe198-1d5f-46fc-af5e-e9cd10bcc14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_spaces = {\"maxIter\": hp.quniform(\"maxIter\", 1, 100, 1),\n",
    "                 \"regParam\": hp.uniform(\"regParam\", 0.1, 10),\n",
    "                 \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a9966d-c2da-46bb-a251-06213916ec80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Finally let's run the hyperparameter tuning with HyperOpt:</p>\n",
    "<p><i>As we are using a model from MLlib, we are going to use <code>Trials</code> class as value for <code>trials</code> parameter of the <code>fmin</code> function.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5c898f-cb76-4e8b-ab32-8695efd97c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"hyperopt_tips\"):\n",
    "    argmin = fmin(fn=objective,\n",
    "                  space=search_spaces,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=15,\n",
    "                  trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7e6cbe-088f-40e2-9eae-9545e0062169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Parameters of the best model: \", argmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a9418fc-f014-4e9c-9e64-997f0bc22ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>See also <a href=\"https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/hyperopt-distributed-ml.html\" target=\"_blank\">this page</a> or <a href=\"https://customer-academy.databricks.com/learn/course/1522/play/9695/advanced-experiment-demo\" target=\"_blank\">this video</a> to learn more on <b>HyperOpt</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d035eb8-cc74-47d6-aa7d-469b39fcb013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"logshap\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">8. Log and view artifacts like SHAP plots, custom visualizations, feature data, images, and metadata</span></div>\n",
    "<p>Looks like logging <b>SHAP</b> - <i>SHapley Additive exPlanations</i> - works with scikit-learn. So let's quickly train a model with scikit-learn library. For simplicity, let's keep <code>day</code> and <code>time</code> features out.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f360077-6a70-442f-9574-c7f2a0429116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a18cd76-5efc-4b35-a6cd-b17b4c11586b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee848b7e-7b52-45d9-84b2-9a030c20f13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load dataset previously prepared\n",
    "pandas_tips = tips_sdf.toPandas()\n",
    "#\n",
    "# set features dataset\n",
    "pandas_tips_features = pandas_tips.drop([\"tip\", \"day\", \"time\"], axis=1)\n",
    "#\n",
    "# set target\n",
    "pandas_tips_target   = pandas_tips[\"tip\"]\n",
    "#\n",
    "# train test split\n",
    "pd_df_X_train, pd_df_X_test, pd_df_y_train, pd_df_y_test = train_test_split(pandas_tips_features,\n",
    "                                                                            pandas_tips_target,\n",
    "                                                                            test_size=0.33,\n",
    "                                                                            random_state=42)\n",
    "#\n",
    "# fit \n",
    "fitted_rfr_model = RandomForestRegressor().fit(pd_df_X_train, pd_df_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206ff31c-6ee9-45f2-b47c-56d1023e9556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Here is an example of logging SHAP to mlflow:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9574a53e-48ab-429d-9315-a730e3c45bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"shap_tips\"):\n",
    "    mlflow.shap.log_explanation(fitted_rfr_model.predict, pd_df_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c190cf02-6531-4f36-bb1c-396e69864287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Here is an example of logging figure to mlflow:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0431f5-6c8f-4c5a-a639-d60e8c780bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"figure_tips\"):\n",
    "    #\n",
    "    # Generate feature importance plot thanks to feature_importances_ attribute of the RandomForestRegressor model\n",
    "    feature_importances = pd.Series(fitted_rfr_model.feature_importances_, index=pd_df_X_train.columns)\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    #\n",
    "    # Log figure to mlflow\n",
    "    mlflow.log_figure(fig, \"feature_importance_rf.png\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1774797690553258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks-ML-professional-S01c-Advanced-Experiment-Tracking",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
