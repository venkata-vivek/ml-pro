{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec963df-dc75-418d-80db-738e6ff9d6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "#\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, FMRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "import mlflow\n",
    "#\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "#\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7244d8db-bce0-46bd-afe8-6e5832d29525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"mlflow\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d528a83-1e69-4eaa-99be-52ac6033bdf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tips_df = sns.load_dataset(\"tips\")\n",
    "#\n",
    "tips_sdf = spark.createDataFrame(tips_df)\n",
    "#\n",
    "display(tips_sdf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827e4971-d9a8-4ad4-9afa-1f905b9114ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tips_sdf = tips_sdf.selectExpr(\"total_bill\",\n",
    "                               \"tip\",\n",
    "                               \"case when sex = 'Female' then 1 else 0 end as sex\",\n",
    "                               \"case when smoker = 'yes' then 1 else 0 end as smoker\",\n",
    "                               \"case when time = 'Dinner' then 1 else 0 end as time\",\n",
    "                               \"day\",\n",
    "                               \"size\")\n",
    "#\n",
    "train_df, test_df = tips_sdf.randomSplit([.8, .2], seed=42)\n",
    "#\n",
    "ohe_cols = [\"size\", \"day\"]\n",
    "num_cols = [\"total_bill\", \"sex\", \"smoker\", \"time\"]\n",
    "target_col = \"tip\"\n",
    "#\n",
    "string_indexer = StringIndexer(inputCols=ohe_cols, outputCols=[c+\"_index\" for c in ohe_cols], handleInvalid=\"skip\")\n",
    "#\n",
    "ohe = OneHotEncoder()\n",
    "ohe.setInputCols([c+\"_index\" for c in ohe_cols])\n",
    "ohe.setOutputCols([c+\"_ohe\" for c in ohe_cols])\n",
    "#\n",
    "assembler_inputs = [c+\"_ohe\" for c in ohe_cols] + num_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f3cdf4-4db6-4c4c-9d9c-20959bc82832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "glr =       GeneralizedLinearRegression(featuresCol=\"features\", labelCol=target_col, maxIter=10)\n",
    "lrm =       LinearRegression(featuresCol=\"features\", labelCol=target_col)\n",
    "fmr =       FMRegressor(featuresCol=\"features\", labelCol=target_col, stepSize=0.001)\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503136cb-6653-4ded-86c3-d32ff5b00ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "signature = mlflow.models.infer_signature(train_df, train_df[[\"tip\"]]);\n",
    "print(signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c360a494-ce38-4ca9-9194-2231dab0e3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_example = train_df.toPandas().head()\n",
    "input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c013b923-535c-4934-a9f8-3917d0b5602f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"linear-regression\"\n",
    "#\n",
    "with mlflow.start_run(run_name=\"Tip-run\") as run:\n",
    "    #\n",
    "    # define pipeline stages according to model\n",
    "    stages = [string_indexer, ohe, vec_assembler, lrm]\n",
    "    #\n",
    "    # set pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    #\n",
    "    # fit pipeline to train set\n",
    "    model = pipeline.fit(train_df)\n",
    "    #\n",
    "    # manually log model to mlflow\n",
    "    mlflow.spark.log_model(model, model_name, signature=signature, input_example=input_example)\n",
    "    #\n",
    "    # manually log parameter to mlflow\n",
    "    mlflow.log_param(\"maxIter\", 11)\n",
    "    #\n",
    "    # predict test set\n",
    "    pred_df = model.transform(test_df)\n",
    "    #\n",
    "    # evaluate prediction\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    #\n",
    "    # manually log metric to mlflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1888c93-82e1-4610-aa25-ca81805e6487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"tips_evaluation\") as run_parent:\n",
    "    #\n",
    "    # loop on the three regression models\n",
    "    for regression_model in [glr, lrm, fmr]:\n",
    "        #\n",
    "        # get model name\n",
    "        model_name = regression_model.__str__().split(\"_\")[0]\n",
    "        #\n",
    "        # Nest mlflow logging\n",
    "        with mlflow.start_run(run_name=model_name, nested=True) as run:\n",
    "            #\n",
    "            # define pipeline stages according to model\n",
    "            stages = [string_indexer, ohe, vec_assembler, regression_model]\n",
    "            #\n",
    "            # set pipeline\n",
    "            pipeline = Pipeline(stages=stages)\n",
    "            #\n",
    "            # fit pipeline to train set\n",
    "            model = pipeline.fit(train_df)\n",
    "            #\n",
    "            # log model to mlflow\n",
    "            mlflow.spark.log_model(model, model_name, signature=signature, input_example=input_example)\n",
    "            #\n",
    "            # predict test set\n",
    "            pred_df = model.transform(test_df)\n",
    "            #\n",
    "            # evaluate prediction\n",
    "            rmse = evaluator.evaluate(pred_df)\n",
    "            #\n",
    "            # log evaluation to mlflow\n",
    "            mlflow.log_metric(\"rmse\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c2363d-aa7d-43fb-b878-36a22c8ba90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bebdfa5f-80ed-42d4-8b71-de9f252de09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fit pipeline to train set\n",
    "model_lrm_autolog = pipeline.fit(train_df)\n",
    "#\n",
    "# predict test set\n",
    "pred_df = model_lrm_autolog.transform(test_df)\n",
    "#\n",
    "# evaluate\n",
    "evaluator.evaluate(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541fad0b-e6c5-40ef-997c-52058a830a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_mlflow = (spark.createDataFrame(mlflow.search_runs())\n",
    "                      .drop(*['tags.mlflow.source.name',\n",
    "                              'tags.mlflow.databricks.notebookPath',\n",
    "                              'tags.mlflow.user',\n",
    "                              'tags.mlflow.databricks.workspaceURL',\n",
    "                              'tags.mlflow.databricks.cluster.info']))\n",
    "display(output_mlflow.orderBy(desc(\"end_time\")).limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79170a1-f6e7-40de-baf2-4488eb307729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(maxIter, regParam, elasticNetParam, labelCol):\n",
    "    \"\"\"\n",
    "    This train() function:\n",
    "     - takes hyperparameters as inputs (for tuning later)\n",
    "     - returns the rmse score on the test dataset\n",
    "    \"\"\"\n",
    "    # Use MLflow to track training.\n",
    "    # Specify \"nested=True\" since this single model will be logged as a child run of Hyperopt's run.\n",
    "    with mlflow.start_run(nested=True):\n",
    "        #\n",
    "        model_hyperopt = LinearRegression(maxIter=maxIter,\n",
    "                                          regParam=regParam,\n",
    "                                          elasticNetParam=elasticNetParam,\n",
    "                                          labelCol=target_col)\n",
    "        #\n",
    "        evaluator_hyperopt = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "        #\n",
    "        stages = [string_indexer, ohe, vec_assembler, model_hyperopt]\n",
    "        #\n",
    "        # set pipeline\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        #\n",
    "        # fit pipeline to train set\n",
    "        model_rfr_hyperopt = pipeline.fit(train_df)\n",
    "        #\n",
    "        # predict test set\n",
    "        pred_df = model_rfr_hyperopt.transform(test_df)\n",
    "        #\n",
    "        # evaluate\n",
    "        rmse = evaluator_hyperopt.evaluate(pred_df)\n",
    "        #\n",
    "        # log rmse for each child run\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "    #\n",
    "    return model_rfr_hyperopt, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d15fab-de76-4115-b4d5-4eb01abdfe6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\" This function is the function to minimize by hyperopt \"\"\"\n",
    "    #\n",
    "    model, rmse = train_model(maxIter=params[\"maxIter\"],\n",
    "                              regParam=params[\"regParam\"],\n",
    "                              elasticNetParam=params[\"elasticNetParam\"],\n",
    "                              labelCol=target_col)\n",
    "    #\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867592e7-1f71-49d4-828f-c1336b1084c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_spaces = {\"maxIter\": hp.quniform(\"maxIter\", 1, 100, 1),\n",
    "                 \"regParam\": hp.uniform(\"regParam\", 0.1, 10),\n",
    "                 \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69e1c23-f46c-4f98-a490-bc9c97e6fce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"hyperopt_tips\"):\n",
    "    argmin = fmin(fn=objective,\n",
    "                  space=search_spaces,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=15,\n",
    "                  trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f81d9f-0e4c-4550-a52e-e765731a079d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Parameters of the best model: \", argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7325d3-d2be-40e2-8f6b-326dc9125f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf77c5a-b061-4e53-98d2-a696ada4560c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bba394d-a490-4699-84d1-25b5a18b0e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load dataset previously prepared\n",
    "pandas_tips = tips_sdf.toPandas()\n",
    "#\n",
    "# set features dataset\n",
    "pandas_tips_features = pandas_tips.drop([\"tip\", \"day\", \"time\"], axis=1)\n",
    "#\n",
    "# set target\n",
    "pandas_tips_target   = pandas_tips[\"tip\"]\n",
    "#\n",
    "# train test split\n",
    "pd_df_X_train, pd_df_X_test, pd_df_y_train, pd_df_y_test = train_test_split(pandas_tips_features,\n",
    "                                                                            pandas_tips_target,\n",
    "                                                                            test_size=0.33,\n",
    "                                                                            random_state=42)\n",
    "#\n",
    "# fit \n",
    "fitted_rfr_model = RandomForestRegressor().fit(pd_df_X_train, pd_df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb41dcff-c7ae-44d3-a362-10a0b1e0f2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "# # Generate summary plot and save\n",
    "# shap.summary_plot(shap_values, X_test,\n",
    "# show=False)\n",
    "# plt.savefig(\"shap_summary.png\")\n",
    "# # Log as artifact\n",
    "# mlflow.log_artifact(\"shap_summary.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7bce25-a1a8-49de-9046-dd9b79a60d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"shap_tips\"):\n",
    "    mlflow.shap.log_explanation(fitted_rfr_model.predict, pd_df_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad3ec4d-5e94-485d-b044-d748cf7b20b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the artifacts\n",
    "artifact_path = \"dbfs:/databricks/mlflow-tracking/a89510df8d5e43debb832600e5d3723f/e6377081e9e742bd920f830ba7e1fc48/artifacts/model_explanations_shap\"\n",
    "\n",
    "# Load the artifacts directly from DBFS\n",
    "shap_values = np.load( artifact_path + \"/shap_values.npy\")\n",
    "base_values = np.load( artifact_path + \"/base_values.npy\")\n",
    "\n",
    "# Create a SHAP explainer object\n",
    "explainer = shap.Explainer(fitted_rfr_model, pd_df_X_test)\n",
    "\n",
    "# Generate the summary plot\n",
    "shap.summary_plot(shap_values, pd_df_X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35e4426-fc27-44a6-b91d-17acd94a3874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"figure_tips\"):\n",
    "    #\n",
    "    # Generate feature importance plot thanks to feature_importances_ attribute of the RandomForestRegressor model\n",
    "    feature_importances = pd.Series(fitted_rfr_model.feature_importances_, index=pd_df_X_train.columns)\n",
    "    fig, ax = plt.subplots()\n",
    "    feature_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    #\n",
    "    # Log figure to mlflow\n",
    "    mlflow.log_figure(fig, \"feature_importance_rf.png\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "advanced-experiment-tracking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
