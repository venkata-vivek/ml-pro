{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fab4bb-231e-48cf-8ed4-fc15a1b22845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h4 style=\"font-variant-caps: small-caps;font-size:35pt;\">Databricks-ML-professional-S03a-Batch</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c17cec-5d2d-49f0-93d1-5ded2421fda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>\n",
    "<br/>\n",
    "<p>This Notebook adds information related to the following requirements:</p><br/>\n",
    "<b>Batch:</b>\n",
    "<ul>\n",
    "<li>Describe batch deployment as the appropriate use case for the vast majority of deployment use cases</li>\n",
    "<li>Identify how batch deployment computes predictions and saves them somewhere for later use</li>\n",
    "<li>Identify live serving benefits of querying precomputed batch predictions</li>\n",
    "<li>Identify less performant data storage as a solution for other use cases</li>\n",
    "<li>Load registered models with load_model</li>\n",
    "<li>Deploy a single-node model in parallel using spark_udf</li>\n",
    "<li>Identify z-ordering as a solution for reducing the amount of time to read predictions from a table</li>\n",
    "<li>Identify partitioning on a common column to speed up querying</li>\n",
    "<li>Describe the practical benefits of using the score_batch operation</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<p><b>Download this notebook at format ipynb <a href=\"Databricks-ML-professional-S03a-Batch.ipynb\">here</a>.</b></p>\n",
    "<br/>\n",
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6aaf81-c559-44bd-bc70-25852c40193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"batch\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">1. Describe batch deployment as the appropriate use case for the vast majority of\n",
    "deployment use cases</span></div>\n",
    "<ul>\n",
    "<li>For use cases where latency is not a restringent requirement <i>(model doesn't have any requirements in term of speed to generate predictions)</i></li>\n",
    "<li>Data or processing tasks are collected and processed in fixed-size batches</li>\n",
    "<li>The data is collected over a period, and the processing occurs on the entire batch</li>\n",
    "<li>Batch processing typically has higher latency because it waits for a set amount of data or a specific time interval before processing.</li>\n",
    "<li>Can leverage databases and object storage for historical data</li>\n",
    "<li>80-90% of use cases are in batch deployment</li>\n",
    "<li><span style=\"text-decoration:underline\">Example</span>: ETL (Extract, Transform, Load) jobs, nightly data processing, and batch analytics are common use cases for batch deployment.</li></ul>\n",
    "<div style=\"display:block;text-align:center\"><img width=\"500px\" src=\"https://i.ibb.co/rxzz2vS/databricks-ml-pro-latency.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e681ce-93ed-4c38-814e-6d851bb56281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"batchsaves\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">2. Identify how batch deployment computes predictions and saves them somewhere\n",
    "for later use</span></div>\n",
    "<p>Refers to the process of making predictions in a batch deployment scenario and storing the results for future reference or utilization. Let's break down the key components:</p>\n",
    "<ol>\n",
    "<li><b>Batch deployment</b>: In the context of machine learning or data processing, batch deployment refers to a mode of operation where predictions or computations are performed on a set of data collected over a specific period or based on a predefined batch size.</li>\n",
    "<li><b>Computes Predictions</b>: This indicates that the system is generating predictions or results based on the input data. In machine learning, this could involve running a trained model on a batch of input data to produce predictions.</li>\n",
    "<li><b>Save predictions Somewhere</b>: After computing predictions, the results are not immediately discarded. Instead, they are stored or saved in a designated location. This storage could be in databases, files, or any other suitable data storage system.</li>\n",
    "<li><b>Later use</b>: The predictions are saved with the intention of using them at a later time. This could be for various purposes such as analysis, reporting, or serving the predictions to end-users when needed.</li>\n",
    "</ol>\n",
    "<p>In practical terms, the process might involve running a batch job that takes a set of input data, applies a trained model to make predictions, and then stores these predictions in a database, file system, or another storage solution. This approach is common in scenarios where real-time processing is not critical, and predictions can be made on a periodic basis.</p>\n",
    "<p>For example, in a <b>recommendation system</b>, batch deployment might involve processing user interactions over a day and generating personalized recommendations overnight. The computed recommendations would then be saved for use the next day when users interact with the system.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dcba302-be4c-4076-a92a-4bf0b7b4d2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"batchprecomputed\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">3. Identify live serving benefits of querying precomputed batch predictions</span></div>\n",
    "<ol>\n",
    "<li><b>Reduced Latency</b>: Precomputing predictions in batch mode allows the system to process and store results ahead of time. This can lead to lower latency during live serving since the predictions are readily available and don't require real-time computation.</li>\n",
    "<li><b>Scalability</b>: Batch processing can be more efficient for large-scale computations. By precomputing predictions in batches, the system can scale more easily to handle varying workloads during live serving.</li>\n",
    "<li><b>Resource efficiency</b>: Computing predictions in batch mode can be resource-efficient, especially for complex models or large datasets. It allows the system to optimize resource utilization during non-peak hours.</li>\n",
    "<li><b>Consistency</b>: Precomputed batch predictions can offer consistency in results, as they are generated using the same model and data. This is in contrast to real-time predictions, which might be influenced by changes in the model or input data at the moment of serving.</li>\n",
    "<li><b>Offline analysis</b>: Having precomputed predictions enables offline analysis of the results, allowing organizations to gain insights, perform audits, and conduct evaluations without affecting live serving.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd6cebd-bdf8-4311-8d2d-aba1a06cb1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"otherusecases\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">4. Identify less performant data storage as a solution for other use cases</span></div>\n",
    "<p>In the context of batch deployment for machine learning models, where predictions are generated in bulk, it's common to save these predictions for later use.</p><p>For live serving, high-performance databases are often preferred for quick retrieval. However, in certain scenarios like populating emails, where rapid access may not be crucial, less performant data storage options, such as a blob store, can be identified as suitable solutions.</p><p>These storage solutions may not offer the highest performance but are chosen strategically based on the specific needs of use cases like email population, balancing considerations of performance and efficiency.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb137c6-a812-4fe9-b723-aa07ef9aa2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"loadregisteredmodels\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">5. Load registered models with load_model</span></div>\n",
    "<p>Let's see two examples to illustrate this requirement:</p>\n",
    "<ul><li>One model trained using <span style=\"color:orangered\">scikit-learn</span> library</li>\n",
    "<li>One model trained using <span style=\"color:orangered\">MLlib</span> library</li></ul>\n",
    "<p>Then both models will be loaded using <code>mlflow.pyfunc.load_model</code> function and used the same way for prediction on test set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a110c9-0567-4cae-9512-c08076a669b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Load some libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d42b33e-8b38-464f-8132-c60c1789f9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "#\n",
    "import mlflow\n",
    "import logging\n",
    "#\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from databricks import feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a17f196-3a17-4b6e-b0c7-dc89a780d430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"mlflow\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493c1a72-5c5a-4d86-a3bf-1dad8f307290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>Load data into a pandas dataframe</b> <i>(for the sake of simplicity, let's only keep numerical columns)</i>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90352c13-b715-457a-8308-09e6c6844b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diamonds_df = sns.load_dataset('diamonds').drop(columns=['cut', 'clarity', 'color'], axis=1)\n",
    "diamonds_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05149010-aa7c-427b-ad6d-0af49c0d995d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>Let's drop duplicates and separate into train set (67%) and test set (33%)</b>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c32173-999f-4935-95cc-9d68b05bcf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diamonds_sdf = spark.createDataFrame(diamonds_df).dropDuplicates()\n",
    "#\n",
    "# Spark Dataframes\n",
    "test_sdf = diamonds_sdf.orderBy(rand()).limit(int(33*diamonds_sdf.count()/100))\n",
    "train_sdf = diamonds_sdf.subtract(test_sdf)\n",
    "#\n",
    "# Pandas Dataframes\n",
    "test_df = test_sdf.toPandas()\n",
    "train_df = train_sdf.toPandas()\n",
    "#\n",
    "print(f\"Number of rows test set: {test_sdf.count()}\")\n",
    "print(f\"Number of rows train set: {train_sdf.count()}\")\n",
    "print(f\"Sum of count rows of train and test set: {train_sdf.count() + test_sdf.count()}\")\n",
    "print(f\"Total number of rows of initial dataframe: {diamonds_sdf.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c30e4b-65a0-4e75-a7e7-155a5be096a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b><span style=\"color:orangered\">Scikit-learn</span> library:</b></p>\n",
    "<ul>\n",
    "<li>Train model on train set</li>\n",
    "<li>Log model to MLflow</li>\n",
    "<li>Register model</li>\n",
    "<li>Load model using <code>mlflow.pyfunc.load_model</code></li>\n",
    "<li>Predict test set using loaded model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ba297d-f5d5-464c-8465-a83126c38c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features and target dataframes\n",
    "X = train_df.drop('price', axis=1)\n",
    "y = train_df['price']\n",
    "#\n",
    "# train model (is automatically logged to mlflow)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "rf.fit(X, y)\n",
    "#\n",
    "# get latest run_id programmaticaly\n",
    "latest_run_id = mlflow.search_runs().sort_values(by=\"end_time\", ascending=False).head(1)['run_id'][0]\n",
    "#\n",
    "# uri to latest run (by default, artifact_path is 'model')\n",
    "uri_scikit_learn = f\"runs:/{latest_run_id}/model\"\n",
    "#\n",
    "# register latest logged model\n",
    "mlflow.register_model(uri_scikit_learn, name=\"scikit-learn_model\")\n",
    "#\n",
    "# load latest registered model\n",
    "scikit_learn_model = mlflow.pyfunc.load_model(uri_scikit_learn)\n",
    "#\n",
    "# prediction of test set using loaded model\n",
    "pd.DataFrame(scikit_learn_model.predict(test_df.drop('price', axis=1)), columns=['predictions']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcaf843c-eb33-4092-b312-137da9e35220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b><span style=\"color:orangered\">MLlib</span> library:</b> There is an additional step which is to convert input to vector using <code>VectorAssembler</code>. Thus, we need a pipeline and we will log to MLflow the fitted pipeline.</p>\n",
    "<ul>\n",
    "<li>Define vector assembler</li>\n",
    "<li>Define Pipeline</li>\n",
    "<li>Train pipeline on train set</li>\n",
    "<li>Log pipeline as a model to MLflow</li>\n",
    "<li>Register model</li>\n",
    "<li>Load model using <code>mlflow.pyfunc.load_model</code></li>\n",
    "<li>Predict test set using loaded model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376bd9ee-157b-4964-8222-6824c01d8072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set vector assembler parameters\n",
    "assembler_inputs = [c for c in train_sdf.columns if c not in ['price']]\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "#\n",
    "# instantiate model\n",
    "mllib_rfr = LinearRegression(featuresCol=\"features\", labelCol='price')\n",
    "#\n",
    "# define pipeline stages\n",
    "stages = [vec_assembler, mllib_rfr]\n",
    "#\n",
    "# set pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "#\n",
    "# fit pipeline to train set\n",
    "model_mllib = pipeline.fit(train_sdf)\n",
    "#\n",
    "# get latest run_id programmaticaly\n",
    "latest_run_id = mlflow.search_runs().sort_values(by=\"end_time\", ascending=False).head(1)['run_id'][0]\n",
    "#\n",
    "# uri to latest run (by default, artifact_path is 'model')\n",
    "uri_mllib = f\"runs:/{latest_run_id}/model\"\n",
    "#\n",
    "# register latest logged model\n",
    "mlflow.register_model(uri_mllib, name=\"mllib_model\")\n",
    "#\n",
    "# load latest registered model\n",
    "mllib_model = mlflow.pyfunc.load_model(uri_mllib)\n",
    "#\n",
    "# Here predictions can be done using same input as for model trained using scikit learn library\n",
    "pd.DataFrame(mllib_model.predict(test_df.drop('price', axis=1)), columns=['predictions']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8847c4cc-4b9a-4789-8394-fab399d94983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"deploysinglenode\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">6. Deploy a single-node model in parallel using spark_udf</span></div>\n",
    "<p>With the model trained using scikit-learn library, it is possible to load it and affect it to a Spark UDF function to make predictions:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e309f1ac-9d80-4c0c-85ac-7df9f314710c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load model into a spark udf\n",
    "predict_scikit_learn = mlflow.pyfunc.spark_udf(spark, uri_scikit_learn)\n",
    "#\n",
    "# make predictions on the spark test dataframe\n",
    "display(test_sdf.withColumn(\"prediction\", predict_scikit_learn(*[c for c in test_sdf.columns if c not in ['price']])).select(\"price\", \"prediction\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3e9181-55f3-4134-bb15-6b9248d1d5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"zorder\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">7. Identify z-ordering as a solution for reducing the amount of time to read predictions\n",
    "from a table</span></div>\n",
    "<p><b>Z-Ordering</b>: colocates related information in the same set of files</p>\n",
    "<p><b>Z-Ordering</b> is a form of multi-dimensional clustering that colocates related information in the same set of files. It reduces the amount of data that needs to be read. <a href=\"https://docs.databricks.com/delta/optimizations/file-mgmt.html#z-ordering-multi-dimensional-clustering\" target=\"_blank\">See more here</a>.</p>\n",
    "<p>Here after is an example of use of Z-ordering.</p>\n",
    "<p>Let's first write a dataframe as a Delta table:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43526c33-a6e1-43d7-9520-41f7ab1f7558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(train_sdf.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"overwriteSchema\", \"true\")\n",
    "          .saveAsTable(\"train_set_diamonds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a61070c-4099-4712-9c2a-8540c5afa9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's get table location:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8164ea2-5abe-45de-9625-66297e69871f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"describe table extended train_set_diamonds\").filter(\"col_name in ('Location')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71cb898e-8da4-456d-acb8-a55f71ab6ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's <b>Z-order</b> table by feature <code>carat</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a61c82-7059-40c4-8a56-116d86d7339b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_partitioned_path = \"dbfs:/user/hive/warehouse/train_set_diamonds\"\n",
    "#\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_partitioned_path}` ZORDER BY (carat)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ce229b-e291-4ab6-ac18-2c36749969e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/train_set_diamonds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb2a00bd-a465-4ce9-8363-9ff1f189f5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"partitioning\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">8. Identify partitioning on a common column to speed up querying</span></div>\n",
    "<p><b>Partitioning</b>: stores data associated with different categorical values in different directories</p>\n",
    "<p><b>Partition</b> will create as many folders as there are distinct values in the specified column for partitioning. Thus, column with high cardinality are not recommanded as partition key.</p><p>Here after is an example of use of Partitioning.</p>\n",
    "<p>Let's first reload the original dataframe and save it as a managed Delta table:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba3aed9-a669-4f36-bb12-818f98871a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.createDataFrame(sns.load_dataset('diamonds')).write\n",
    "                                                    .format(\"delta\")\n",
    "                                                    .mode(\"overwrite\")\n",
    "                                                    .option(\"overwriteSchema\", \"true\")\n",
    "                                                    .saveAsTable(\"diamonds_df_not_partitioned\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860d0033-92f9-44d1-87f6-c37eccf3b835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's have a look at the content of the delta table folder. We see that there are four parquet files an a folder <code>_delta_log</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e450e873-f459-4e17-9c4b-531b438f3d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"describe table extended diamonds_df_not_partitioned\").filter(\"col_name in ('Location')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c435ce0b-f242-4d60-a2e4-60ab6649056a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(\"dbfs:/user/hive/warehouse/diamonds_df_not_partitioned\"):\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a110cb9-e2fd-44f3-9701-06dde864d68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>We can identify the feature <code>cut</code> as a good candidate to be the partition key:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3129c2fe-338c-4478-ac5e-9634f7fdd2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"diamonds_df_not_partitioned\").groupBy(\"cut\").count().orderBy(desc('count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac6e9f96-bf12-4291-b243-0d782ea17ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's partition using <code>cut</code> as partition key:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdab33b6-54d0-41d7-bbab-78f1d6dc801f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"diamonds_df_not_partitioned\")\n",
    "      .write.partitionBy(\"cut\")\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .saveAsTable(\"diamonds_df_partitioned\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fad799d-4524-40eb-a7c0-77ae6da2175d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Now let's have a look at the content of the partitioned table. We see there as many folders as there are distinct values in column <code>cut</code>. This will speed-up requests.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d003f526-9e67-44f4-9500-fc80c99a39e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls(\"dbfs:/user/hive/warehouse/diamonds_df_partitioned\"):\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "088d26da-3f3c-400e-b86c-a9c5900e7c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<a id=\"scorebatch\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">9. Describe the practical benefits of using the score_batch operation</span></div>\n",
    "<p><code>score_batch</code> let's make predictions easily on a large amount of data at a time using features coming from <b>feature store</b>.</p>\n",
    "<p>Let's have a look at an example to illustrate this requirement:</p>\n",
    "<ol>\n",
    "<li><b>Load dataset</b>: The dataset used for the example is <code>diamonds</code> dataset from <b>Seaborn</b> library</li>\n",
    "<li><b>Create Feature table in Feature Store</b></li>\n",
    "<li><b>Push preprocessed features to Feature Store</b></li>\n",
    "<li><b>Create train and test sets</b></li>\n",
    "<li><b>Prepare and train models</b></li>\n",
    "<li><b>Log models to associate them to features in Features Store</b></li>\n",
    "<li><b>Score models on test set using <code>score_batch</code></b></li>\n",
    "<li><b>Case of new data</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e8c666e-4e83-481e-ba1c-9171de353f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>1. Load dataset</b></p>\n",
    "<ul>\n",
    "<li>Starting from the <code>diamonds</code> dataset from <b>Seaborn</b> library</li>\n",
    "<li>Set an <code>index</code> column made of unique values. It will be used as <b>primary key</b> for the <b>Feature Store</b>.</li>\n",
    "</ul>\n",
    "<i>Looks like there is a problem with having a column named <code>x</code> or <code>X</code>... let's rename.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3116320-dbb9-4aea-b8d4-b826acdfc39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_diamonds = sns.load_dataset('diamonds').reset_index()\n",
    "#\n",
    "diamonds_full = spark.createDataFrame(pd_diamonds).withColumnRenamed('x', 'x_r')\n",
    "#\n",
    "display(diamonds_full.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f71306-b829-4150-8562-2d7cc4728b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>2. Create Feature table in Feature Store</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92306fc4-ca06-46bb-9aa2-f49eb4c3e6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a feature store client\n",
    "fs = feature_store.FeatureStoreClient()\n",
    "#\n",
    "# fs.drop_table(\"default.diamonds_fs\")\n",
    "#\n",
    "# create feature table - as only the scema is provided in the command below, it will only create the table structure without populating it with data\n",
    "result = fs.create_table(name=\"diamonds_fs\",                          # required\n",
    "                         primary_keys=[\"index\"],                      # required\n",
    "                         schema=diamonds_full.drop(\"price\").schema,   # need either dataframe schema\n",
    "                         #df=diamonds_full,                           # or dataframe itself\n",
    "                         description=\"seaborn diamonds dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e64855c-5e9e-4ebc-bd3a-168ec46d9758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>3. Push preprocessed features to Feature table</b></p>\n",
    "<i>(There's no preprocessing done there for the sake of simplicity. Ideally, features pushed to Feature Store should be processed and ready to be used for model training)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3f95e8-ebae-474a-8372-dd3043a63f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs.write_table(name=\"diamonds_fs\",\n",
    "               df=diamonds_full.drop('price'),\n",
    "               mode='merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9215ba4b-6e84-451a-ba95-d3e012c12ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>After that, <b>Feature table</b> is available in <b>Features</b> menu as well the associated Delta table in <b>Catalog</b> menu.</p>\n",
    "<img src=\"https://i.ibb.co/mzDxNc9/featurestore.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96a564b-f4ac-423f-bfde-7a1ec12fa25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>4. Create train and test sets</b></p>\n",
    "<p>Here, features are now available in <b>Features Store</b>. It is possible to load them from there to train a model. For this example, we will:</p>\n",
    "<ul>\n",
    "<li>Train two different models:\n",
    "    <ul>\n",
    "       <li>one trained using <b>4 features</b>: <code>x_r</code>, <code>y</code>, <code>z</code>, <code>carat</code></li>\n",
    "       <li>another one trained using <b>all numerical features</b></li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Train models using a part of the dataset</li>\n",
    "<li>Evaluate them on another part of the dataset which was not used for training</li>\n",
    "</ul>\n",
    "<p>What will help to make the difference between the train set and test set is the <b>Primary key</b> column: <code>index</code>.</p>\n",
    "<p>Moreover, later for scoring by using the test set, we will need the initial target values from initial <code>price</code> column. Thus, the columns needed for the train and test sets are: <code>index</code> and <code>price</code>.</p>\n",
    "<ul>\n",
    "<li>with <code>index</code> we retrieve from Feature store the rows needed to train/test the models</li>\n",
    "<li>with <code>price</code> we have the target used to train/evaluate the models</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1317f3aa-58c4-4c96-9744-ce965b4bd720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_test  = diamonds_full.select(\"price\", \"index\").orderBy(rand()).limit(int(33*diamonds_full.count()/100))\n",
    "y_train = diamonds_full.select(\"price\", \"index\").subtract(y_test)\n",
    "#\n",
    "display(y_train.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b207fd0e-c0f0-4dbe-a031-91d8c9744087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's create the Feature Store training sets.</p>\n",
    "<ul><li>They will be useful now to load from Feature Store the features needed to train each models</li>\n",
    "<li>They will be useful later when logging the trained models to Feature Stores for easy tracking.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609d2917-17b5-467c-8d2b-7a8fbd1beb36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Four features training set:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d75cc8-2c72-4e8e-be93-46b2052dd79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With 4 features: x, y, z, carat\n",
    "feature_lookups_4_features = [feature_store.FeatureLookup(table_name=\"diamonds_fs\",\n",
    "                                                          feature_names=['x_r', 'y', 'z', 'carat'],\n",
    "                                                          lookup_key=\"index\")]\n",
    "#\n",
    "# create associated training set\n",
    "train_set_4_features = fs.create_training_set(y_train,\n",
    "                                              feature_lookups_4_features,\n",
    "                                              label=\"price\",\n",
    "                                              exclude_columns=\"index\")\n",
    "#\n",
    "# load training set\n",
    "train_set_4 = train_set_4_features.load_df()\n",
    "#\n",
    "# display to check\n",
    "display(train_set_4.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d2a88a-d33c-44cb-80a0-143e574891bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>All numerical features training set:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d56d779-8327-4bdd-b9e7-859c60d9d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With all numerical features\n",
    "feature_lookups_all_features = [feature_store.FeatureLookup(table_name=\"diamonds_fs\",\n",
    "                                                            feature_names=[c for c in diamonds_full.columns if c not in ['index', 'cut', 'clarity', 'price', 'color']],\n",
    "                                                            lookup_key=\"index\")]\n",
    "#\n",
    "# create associated training set\n",
    "train_set_all_features = fs.create_training_set(y_train,\n",
    "                                                feature_lookups_all_features,\n",
    "                                                label=\"price\",\n",
    "                                                exclude_columns=\"index\")\n",
    "#\n",
    "# load training set\n",
    "train_set_all = train_set_all_features.load_df()\n",
    "#\n",
    "# display to check\n",
    "display(train_set_all.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8693996-799e-4cae-9e14-b6ba149a1c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>5. Prepare and train models</b></p>\n",
    "<p>Training of two scikit-learn models based on different features sets. Input dataframes need to be pandas dataframes/series.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ddce2d7-694d-42c1-ae9a-478d026a351f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Model trained using the four features dataset:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703f0b5c-bfe3-4c2e-868d-272162750573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_4 = train_set_4.drop(\"price\").toPandas()\n",
    "y_train_4 = train_set_4.toPandas()[\"price\"]\n",
    "#\n",
    "rf_4_model = RandomForestRegressor()\n",
    "#\n",
    "rf_4_model.fit(X_train_4, y_train_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0814a65e-5ff5-4c18-b53a-1fadff6ae268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Model trained using all numerical features dataset:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e4e873-31bd-4c18-874a-f88ce8c54c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_all = train_set_all.drop(\"price\").toPandas()\n",
    "y_train_all = train_set_all.toPandas()[\"price\"]\n",
    "#\n",
    "rf_all_model = RandomForestRegressor()\n",
    "#\n",
    "rf_all_model.fit(X_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838f0a82-3f68-4c36-b235-3ce2f873410a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>6. Log models to associate them to features in Feature Store</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd03ad9-eab5-420a-a198-dd2c90f71ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name_4_features = \"trained_with_4_features\"\n",
    "#\n",
    "fs.log_model(rf_4_model,\n",
    "             artifact_path=model_name_4_features,            # parameter required\n",
    "             flavor=mlflow.sklearn,                          # parameter required\n",
    "             training_set=train_set_4_features,              # either training_set or feature_spec_path parameters required\n",
    "             registered_model_name=model_name_4_features);   # not required. However model will not be linked to features in features store until model is registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44704fb-e0dd-451b-803f-70f7ba6a06c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name_all_features = \"trained_with_all_features\"\n",
    "#\n",
    "fs.log_model(rf_all_model,\n",
    "             artifact_path=model_name_all_features,            # parameter required\n",
    "             flavor=mlflow.sklearn,                            # parameter required\n",
    "             training_set=train_set_all_features,              # either training_set or feature_spec_path parameters required\n",
    "             registered_model_name=model_name_all_features);   # not required. However model will not be linked to features in features store until model is registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61497a33-a9a4-4bf9-88b2-02e0f63f8bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>At this point we can see that models are associated with the features they were trained on in Features Store:</p>\n",
    "<img width=\"1000px\" src=\"https://i.ibb.co/LCrZYKD/featurestore1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3a733f0-123a-4fe7-b497-70b72e58b9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>7. Score models on test set using <code>score_batch</code></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3f591e7-37f6-4ec6-a5d2-27f4db5323ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>Model trained on four features:</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3dac6a5-0caa-4252-8295-40383c5592aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# latest run id for model named \"trained_with_4_features\"\n",
    "for val in mlflow.MlflowClient().get_registered_model(model_name_4_features):\n",
    "    if val[0]=='latest_versions':\n",
    "        run_id_4 = val[1][0].run_id\n",
    "#\n",
    "# uri to latest run\n",
    "uri_4_features = f\"runs:/{run_id_4}/{model_name_4_features}\"\n",
    "print(uri_4_features)\n",
    "#\n",
    "# predict on test set\n",
    "predictions_df_4_features = fs.score_batch(uri_4_features, y_test).select(\"price\", \"prediction\");\n",
    "display(predictions_df_4_features.orderBy(rand()).limit(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a6eaba-d08c-477b-ae20-a4baa09390cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Score RMSE on test set for model trained with 4 features:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df34d1d-3064-4c18-b8b7-a4ff9fb793a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"RMSE for model trained on 4 features:\",\n",
    "      mean_squared_error(predictions_df_4_features.toPandas()['price'], predictions_df_4_features.toPandas()['prediction'], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10058824-419f-4ab9-b8d4-049a99b6db86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>Model trained on all numerical features:</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c4c37f-55a2-463c-9eaa-a737070e550a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# latest run id for model named \"trained_with_all_features\"\n",
    "for val in mlflow.MlflowClient().get_registered_model(model_name_all_features):\n",
    "    if val[0]=='latest_versions':\n",
    "        run_id_all = val[1][0].run_id\n",
    "#\n",
    "# uri to latest run\n",
    "uri_all_features = f\"runs:/{run_id_all}/{model_name_all_features}\"\n",
    "print(uri_all_features)\n",
    "#\n",
    "# predict on test set\n",
    "predictions_df_all_features = fs.score_batch(uri_all_features, y_test).select(\"price\", \"prediction\", \"carat\");\n",
    "display(predictions_df_all_features.select(\"price\", \"prediction\").orderBy(rand()).limit(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1553678a-e273-4b23-91ed-568c901ab53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Score RMSE on test set for model trained with all numerical features:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97dab406-3bc9-4a07-9f51-02471704d3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(mean_squared_error(predictions_df_all_features.toPandas()['price'], predictions_df_all_features.toPandas()['prediction'], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48be5619-0c6d-4915-b1b5-590396271f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Comparison of actual price and predicted price according to <code>carat</code> for model trained using all numerical features, on a sample of 1000 random entries:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53c8f7e-9a5b-424f-9ba9-283685d3ad54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 6))\n",
    "#\n",
    "sample_predictions = predictions_df_all_features.orderBy(rand()).limit(1000)\n",
    "#\n",
    "plt.scatter(list(sample_predictions.toPandas()['carat']), list(sample_predictions.toPandas()['price']), label='Actual Price', color='blue', marker='o')\n",
    "plt.scatter(list(sample_predictions.toPandas()['carat']), list(sample_predictions.toPandas()['prediction']), label='Prediction', color='orange', marker='s')\n",
    "#\n",
    "# Adding labels and title\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Prices')\n",
    "plt.title('Actual Price vs Prediction')\n",
    "#\n",
    "# Adding grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "#\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "#\n",
    "# Show plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0bcb9d4-60d8-49f9-9af6-9d706b2badb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><b>8. Case of new data</b></p>\n",
    "<p>How to predict newly arriving data if there is no information on it in Feature Store? Need to update first the Feature Store with new data:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b86ebd5-e014-43e9-882b-3e8dde08b4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Let's create a new diamond data. Schema should match data in Feature Store:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b297af-6a07-448a-9342-e09d5847cc28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diamonds_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f332ab5d-e77c-4880-bf10-1c0863f228a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_diamond = (diamonds_full.limit(1).withColumn('index',   lit(88887777).cast('long'))\n",
    "                                     .withColumn('carat',   lit(2).cast('double'))\n",
    "                                     .withColumn('cut',     lit('Good').cast('string'))\n",
    "                                     .withColumn('color',   lit('E').cast('string'))\n",
    "                                     .withColumn('clarity', lit('VS1').cast('string'))\n",
    "                                     .withColumn('depth',   lit(40).cast('double'))\n",
    "                                     .withColumn('table',   lit(64).cast('double'))\n",
    "                                     .withColumn('x_r',     lit(4.14).cast('double'))\n",
    "                                     .withColumn('y',       lit(3.5).cast('double'))\n",
    "                                     .withColumn('z',       lit(2.1).cast('double')))\n",
    "#\n",
    "new_diamond_with_price = spark.createDataFrame(pd.DataFrame({'index': [88887777], 'price': [4500]}))\n",
    "#\n",
    "new_diamond_without_price = spark.createDataFrame(pd.DataFrame({'index': [88887777]}))\n",
    "#\n",
    "diamond_unknown = spark.createDataFrame(pd.DataFrame({'index': [98989898]}))\n",
    "#\n",
    "display(new_diamond)\n",
    "display(new_diamond_with_price)\n",
    "display(new_diamond_without_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69917373-02c9-46f1-adc7-73a283e1505b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>Now, update the Feature Store with the new diamond data:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9f4763-9446-41a4-b963-e51c0d6fcb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs.write_table(name=\"diamonds_fs\",\n",
    "               df=new_diamond,\n",
    "               mode='merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdaea63c-b435-4f75-8cf0-aec1b2991b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>We verify that <code>score_batch</code> predicts either with/without the price of the new data, the only requirement is the <b>primary key</b> - in this particular case, <b>column <code>index</code></b> - of the new diamond data in the Feature Store:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bed0bc-f4d4-4f22-95ca-83d4a56c4c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict with price\n",
    "predictions_new_diamond_with_price = fs.score_batch(uri_all_features, new_diamond_with_price)\n",
    "display(predictions_new_diamond_with_price)\n",
    "#\n",
    "# predict without price\n",
    "predictions_new_diamond_without_price = fs.score_batch(uri_all_features, new_diamond_without_price)\n",
    "display(predictions_new_diamond_without_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88f5654a-c3c1-4b83-a268-d8d67bd94906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p>And verify that if a primary key is not found in the Feature Store, it results in an error:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac28dd7-81a6-4ea8-bec1-b779aaa14d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict unknown diamond\n",
    "predictions_unknown_diamond = fs.score_batch(uri_all_features, diamond_unknown)\n",
    "display(predictions_unknown_diamond)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1612971859527038,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks-ML-professional-S03a-Batch",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
